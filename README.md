# Syracuse
Applied Data Sciences Master's Degree Projects


MBC 638 – Data Analysis and Decision Making 
As my introduction to the Applied Data Sciences program, MBC 638 was a great refresher course into statistical thinking and problem-solving. I have been through many statistics courses in my education experience, so this class was more of a reminder as to how these processes apply to everyday business processes, as well as process improvement. 
We were asked to find a process in our lives of which had room for improvement. The goal was to gather information on said process, brainstorm ideas as to how we may be able to improve this process, then implement solutions and track the differences. I chose to lessen food waste within my home and try to minimize the amount of food/monetary waste my wife and I produced in a week. 
To gather the data necessary, I saved grocery store receipts and entered the purchased items, their quantities (1 pack of Spinach, 4 apples, etc.), and their prices into Excel. After gathering the data, I revisited these items upon them expiring or being consumed and would log the amount leftover as well as the “money wasted” based on the leftover weight divided by the total price. After about a month of this analysis I then implemented what I believed to be the best solutions which were to stop over-purchasing chicken (our most expensive food waste item) as well as start making smoothies to clear out any leftover produce before it spoils. 
These solutions yielded excellent results on our food waste statistics, which was visualized by an R-Bar chart and further explained via descriptive stats and a two-sample t-test. Upon reaching the conclusion that my solutions had a statistically significant effect on our monetary waste per week, it then became my goal to maintain this level of food waste as the new baseline and find further improvements to keep improving. 


IST 659 – Database Administration Concepts and Database Management
	My introduction to data-centered coding languages within this program took place within IST 659. Although I had been learning SQL on my own time prior to this program, it became clear that there was still much to be learned when it comes to building an actual database. 
IST 659 Data Admin Concepts and Database management was a course centered around the basics behind creating, updating, and maintaining a database for a business. We were immediately introduced to the dimensional modeling guidelines and tested on our abilities to create a relational database given a business’ needs. For my project I chose to create a fictional computer assembly business; this business was small at the time and therefore did not have any means of properly detailing its orders, inventory on hand, and customer profiles. 
	The beginning steps were to map out the relationships between the many parts of the business. By utilizing one-to-one, one-to-many, and many-to-one relationships, I was able to create a coherent relationship diagram that spanned from the parts store that Ralph ordered his inventory from to the customer being delivered their PC. Inventory relationships quickly became a difficult task for an inexperienced relational database mapper such as myself, there are many parts that go into a functioning PC, and each part had 3 possible tiers of pricing depending on the customer’s needs. My professor, Doctor Block, was a massive help in guiding me through the process and the map was eventually sound. 
	Next in the process was to create the tables and link them all together via primary and foreign keys. This was a straightforward process because of the attention to detail needed to complete step one. After creating the tables, it was then time to actually input data and create the functions that Ralph needed to speed up his business. These functions included inventory additions and subtractions, as well as views that allowed Ralph to quickly determine his most popular products. Various visualizations were created to help answer the business questions at hand, and a report detailing every step of the process was created. 


IST 687 – Intro to Data Analysis: Portuguese Students’ Grade Analysis
	My true introduction to Rstudio within this program took place within IST 687 which hosted many different statistical techniques. The group project within this class expected us to find a medium-large dataset on the topic of our choosing and seek out any relationships between the input and output variables that may exist. Our dataset was a collection of student home-life data as well as their grades in math and English classes. Home-life variables included things like extracurricular activities, does the student drink or smoke, student age, student sex, are the student’s parents still together, and so on. The number of students present within the data allowed for finely-tuned statistical tools such as RandomForest to be used and create meaningful analysis. 
	My work specifically centered around using RandomForest to determine which variables were the most important in determining student success or failure. I separated the student pool into those that passed their classes in all 3 measured semesters, as well as those students who failed all 3 semesters. Upon creating this split, I was able to run a RandomForest test to lay out which variables had the most sway over the end result. Although there were some interesting variables that I hadn’t expected to make an appearance, the end result was clear – student success begins at home. This may be an adage as old as education itself, however, seeing a sophisticated statistical program reaffirm this was an eye opener for me on my data analysis journey. This was one of the first instances of having a preconceived notion and seeing firsthand the work needed to reproduce that notion with data cleaning statistics.


IST 707 – Data Analysis	
	Serving as a great follow-up to IST 687, IST 707 was a much deeper dive into analysis techniques available to us via Rstudio. The complexity level behind the statistical tests was cranked up a notch, and some serious coding practice became necessary. We focused on data prep, descriptive statistics, and various algorithms that shed light on the intricacies of datasets that cannot be found easily. 
	My project focused on a large dataset of Beer types, measurements, and their review scores; a surprisingly scientific set of variables playing into a ranking number between 0 and 5. The measurements included things such as ABV, Brewery, beer style, mouthfeel, aroma, the list goes on. Not only were the variables numerous, the number of types of beer totaled ~3000. 
	I began the project with data cleaning and trimming, such as re-naming columns and trimming the entries that had fewer than 100 reviews. I quickly realized that there were many different naming conventions to beer styles (IPA – American, IPA – Belgian, etc.) and determined that the analysis might suffer if the category types were too spread out. Creating larger buckets for the different styles allowed for a better result and more applicable findings for those that are interested. Being able to average all IPA, Stout, Pilsner, and Lager results together, just to name a few, granted me the ability to make better suggestions. 
	I then created descriptive statistics to paint a broad-strokes picture of how the dataset measured, and what to be on the lookout for regarding any anomalies that may be present. This part of the analysis quickly alerted me to the idea that method bias may be impacting the results of this dataset. The mean review score was a 3.8/5, which seemed awfully high to me; were people more generous with their scores due to the alcohol inhibiting their judgement, as well as drinking beer being an activity that elicits happiness regardless of the quality of product? That was at least my determination at the time, but the statistical measurements to follow were still insightful regardless of the possible method bias. 
	My further analysis began with an Apriori Rules analysis which linked commonly occurring relationships between variables within the data. This test allowed for interesting findings such as Anheuser-Busch typically having low scores to appear, as well as which styles were typically found on the higher or lower side of the score spectrum. 
	I then performed an in-depth RandomForest analysis on the dataset to read the variables present and predict a beer’s score based on the beer’s name, style, brewery, ABV, and number of reviews. The test was able to yield a 69% accuracy on predicting a review score based on these variables, with the variable importance plot showing that number of reviews, beer name, and brewery had the most impact on predictive accuracy.


IST 719 – Information Visualization 
	At this point in the program, I began feeling very comfortable with data gathering, cleaning, and analysis; next on the agenda would be visualization! This course focused on data visualization via Rstudio, with many different charting techniques taught throughout and an emphasis on customization via Adobe Illustrator. My final project revolved around political donations within the American Sports world (MLB, NBA, NFL, WNBA, NASCAR, etc.), and visualized the differences each league had in where their political donations were sent to based on republican, democratic, or independent political alignment. 
	The visualizations I used involved relatively basic pie and bar charts, however, the main visualization utilized the ggmap package of Rstudio and plotted the top 10 donating teams in the country along with their sum of donations represented by the size of their point on the map. This map took up most of the project working hours, followed by attempting to perfect the theme in Illustrator. Looking back, there are certainly some details I would change now; however, at the time I was very happy with the results. 


IST 652 – Scripting for Data Analysis
	My Rstudio analysis options had all but been exhausted at this point, so the next logical step was to begin classes centered around Python instead. Before IST 652, I had zero experience working with Python, so there was a large learning curve to this class. Our project was expected to utilize Python data cleaning, analyzing, and visualizing techniques taught through the course. We were also expected to gather our data from multiple sources; I chose to analyze Tesla stock price data from its conception to current day, with an added twist on the variables playing into its price action. 	
	I gathered my data from both Kaggle and Yahoo Finance, which conveniently lets you download any given stock’s pricing data in many different formats depending on your needs. I had gathered Tesla’s information, as well as the NASDAQ market index price info for something to compare Tesla stock with. After cleaning and grouping the data into monthly slices, I was able to create a browser-based line graph that allowed for zooming in and out on specific moments. The twist mentioned above was that I wanted to determine which events lead to the largest changes in Tesla’s stock price, including the often-controversial statements made by their CEO, Elon Musk, on his Twitter account. 
In order to gather his tweets, I found a Python package called snscrape that allows you to scrape a given user’s tweets without needing API privilege. Upon having a functioning script, I had gathered ~10,000 tweets of Elon’s and selected the ones that were universally deemed as the tweets that “absolutely had an effect on Tesla’s stock price.” For Tesla’s price line graph annotated by Elon’s tweets I found a way to create a candlestick chart similar to those you may see on stock trading apps and websites. Combining all of these elements made for a nice graph in my opinion, and definitely provided enough material for an interesting presentation. 


IST 718 – Big Data Analysis
	Now that I had some python experience, the best next step was to take Big Data Analysis which is a Python based course centered around large datasets and advanced statistical analysis. We used many different predictive models and analytical techniques I had previously not even considered doing through a language such as Python. My group project was titled “Tornado Alley Analysis,” and we set out to determine if the current definition of Tornado Alley was outdated.
	The data was gathered through multiple sources of historical Tornado data, and the cleaning process was lengthy in order to properly combine these data sets. We ended up with just under 30,000 rows of entries with 19 columns worth of variables. My role in the project was to create regression tests in order to determine the most important variables that had effects on total damage, total injuries, and total deaths throughout the dataset. 
	My next objective was to create an LTSM test to hopefully predict the total damage, injuries, and deaths. This test ended up performing poorly for these categories, however, I ran the test on the F-Scale measurement of all entries and managed to have great results. 
With as many entries as we had, I quickly realized that statistical tests can be both demanding on your hardware as well as your time. Some of these regression/LTSM tests would end up taking all night to produce results, and they did not always work either! This was a great learning experience in trimming datasets for testing in order to see if your script actually works; once functionality was proven, running the script on the main dataset at the right time in order to maximize efficiency. 
We ended up recommending that Tornado Alley’s population of states should be expanded to include more than just Kansas, Oklahoma, and Texas. Rather, we felt that there should be more awareness nationwide that every state is susceptible to tornadoes, and if the trends we saw within the data were true, then eventually every state could be considered a hot spot. 
